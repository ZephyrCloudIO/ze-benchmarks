# scenario.yaml Template
# Customize this template for your specific benchmark scenario

# Basic Information (Required)
id: my-scenario
suite: my-suite
title: "My Scenario Title"
description: |
  Describe what this scenario tests and what the agent needs to accomplish.

  Success criteria:
  - List the key requirements for successful completion
  - Include specific technologies, configurations, or outcomes expected
  - Define what "passing" looks like for this benchmark

# Workspace Configuration (Required for code-based scenarios)
# For artifact-based scenarios (e.g., Figma), set required: false
workspace:
  required: true  # Set to false for artifact-based scenarios
  node: "20.x"
  manager: auto
  managers_allowed: [pnpm]
  workspaces: none  # Use "pnpm", "npm", "yarn", or "none"

# Artifact Configuration (Optional - for artifact-based scenarios)
# Use this instead of workspace for scenarios that evaluate external artifacts
# artifact:
#   type: figma  # Options: figma, api, image
#   figma_file_id: "abc123xyz456"  # Extract from Figma URL: figma.com/file/{file_id}/...
#   figma_file_key: "optional-version-key"  # Optional: specific file version

# Constraints (Optional)
# Restrict which package managers the agent can use
constraints:
  managers_allowed: [pnpm]

# Validation Commands (Required for code-based scenarios)
# For artifact-based scenarios, set required: false
validation:
  required: true  # Set to false for artifact-based scenarios
  commands:
    install: "pnpm install"
    build: "pnpm build"
    test: "pnpm test"  # Optional: remove if no tests
    lint: "pnpm lint"  # Optional: remove if no linting

# Oracle Answers (Optional)
# File containing expected responses to common agent questions
oracle:
  answers_file: "./oracle-answers.json"

# Heuristic Checks Configuration (Optional)
# Hard-coded validation checks that don't require LLM evaluation
# Useful for automated, deterministic checks across different scenario types
heuristic_checks:
  enabled: true

  # Command-based checks (for coding scenarios)
  # Run commands and check exit codes
  commands:
    - name: "build_succeeds"
      command: "pnpm build"
      weight: 2.0
      description: "Verify build completes without errors"

    - name: "tests_pass"
      command: "pnpm test"
      weight: 2.0
      description: "Verify all tests pass"

    - name: "lint_passes"
      command: "pnpm lint"
      weight: 1.0
      description: "Verify code meets linting standards"

  # File existence checks
  # Verify specific files or directories exist
  files:
    - name: "config_exists"
      path: "next.config.js"
      weight: 1.0
      description: "Next.js config file should exist"

    - name: "readme_exists"
      path: "README.md"
      weight: 0.5
      description: "Project should have README"

  # Pattern/content checks (for design scenarios)
  # Search for regex patterns in files (supports glob patterns)
  patterns:
    - name: "uses_primary_color"
      file: "src/styles/**/*.{ts,tsx,css}"
      pattern: "#3B82F6|#2563EB|rgb\\(37,\\s*99,\\s*235\\)"
      weight: 1.5
      description: "Check if primary blue color is used"

    - name: "has_dark_mode"
      file: "src/**/*.{ts,tsx}"
      pattern: "dark:|darkMode:|theme\\.dark"
      weight: 1.0
      description: "Check for dark mode implementation"

    - name: "uses_accessibility"
      file: "src/components/**/*.tsx"
      pattern: "aria-|role=|alt="
      weight: 1.0
      description: "Check for accessibility attributes"

  # Structured data checks (for product scenarios)
  # Validate JSON/YAML structure or check for sections in documents
  structured:
    - name: "api_spec_complete"
      file: "api-spec.json"
      json_path: "$.paths./users.get"
      exists: true
      weight: 1.5
      description: "API spec should define GET /users endpoint"

    - name: "package_has_scripts"
      file: "package.json"
      json_path: "$.scripts.test"
      exists: true
      weight: 1.0
      description: "package.json should have test script"

    - name: "security_section_exists"
      file: "docs/PRD.md"
      section_header: "## Security Requirements"
      weight: 2.0
      description: "PRD must have Security Requirements section"

    - name: "testing_section_exists"
      file: "docs/PRD.md"
      section_header: "## Testing Strategy"
      weight: 1.5
      description: "PRD should document testing approach"

  # Custom script checks (for complex validations)
  # Execute custom scripts with arguments (supports template variables)
  scripts:
    - name: "figma_color_palette"
      script: "./scripts/check-figma-colors.js"
      args: ["--file-id", "${artifact.figma_file_id}"]
      weight: 1.5
      description: "Verify Figma design uses correct color palette"

    - name: "component_structure"
      script: "./scripts/validate-components.sh"
      args: ["src/components"]
      weight: 1.0
      description: "Validate component file structure"

# LLM Judge Configuration (Optional)
# AI-powered evaluation using language models
# REQUIRED: Define custom evaluation categories tailored to your scenario
# Each category will be evaluated on a 1-5 scale and averaged equally
llm_judge:
  enabled: true
  model: "anthropic/claude-3.5-sonnet"
  temperature: 0.1
  max_tokens: 2000
  # Categories: Array of evaluation criteria (each scored 1-5)
  # Include weight percentages in the category text if desired
  # Example format: "Category Name (Weight: 25%): Description and scoring guidance"
  categories:
    - "Code Correctness (Weight: 30%): Implementation accuracy, proper API usage, no bugs. Score 1-5: 1=broken code, 5=perfect implementation"
    - "Best Practices (Weight: 25%): Following framework conventions, clean code patterns. Score 1-5: 1=violates standards, 5=exemplary practices"
    - "Technical Execution (Weight: 20%): Proper use of tools, correct commands, error handling. Score 1-5: 1=incorrect execution, 5=robust implementation"
    - "Documentation & Communication (Weight: 15%): Clear explanations, proper comments. Score 1-5: 1=no documentation, 5=comprehensive docs"
    - "Overall Quality (Weight: 10%): Holistic assessment combining all aspects. Score 1-5: 1=failure, 5=exceptional success"

# Scoring Weights (Optional)
# Customize how different evaluators contribute to the final score
rubric_overrides:
  weights:
    install_success: 2.0          # Dependencies install successfully
    tests_nonregression: 1.0      # Tests pass
    manager_correctness: 2.0      # Correct package manager used
    dependency_targets: 0.5       # Target dependencies updated
    integrity_guard: 1.0          # No security vulnerabilities
    heuristic_checks: 2.0         # Heuristic validation checks
    llm_judge: 3.0                # AI evaluation score

# =============================================================================
# Additional Optional Sections
# =============================================================================

# Baseline Commands (Optional)
# Run before agent execution to establish baseline state
# baseline:
#   run:
#     - cmd: "pnpm install"
#     - cmd: "pnpm test"

# Timeout (Optional, default: 30 minutes)
# timeout_minutes: 40

# Dependency Targets (Optional)
# Specify which dependencies should be updated
# targets:
#   required:
#     - name: "react"
#       to: ">=18.3.0 <19.0.0"
#   optional:
#     - name: "eslint"
#       to: "^9.0.0"

# Advanced Constraints (Optional)
# constraints:
#   blocklist:
#     - name: "webpack"
#       reason: "Pinned by build system"
#   namespace_migrations:
#     - from: "xterm"
#       to: "@xterm/xterm"
#   companion_versions:
#     - main: "react"
#       companions:
#         - name: "@types/react"
#           rule: "major must match"

# Additional Rubric Weights (Optional)
# rubric_overrides:
#   weights:
#     used_manager_commands: 0.6
#     semantic_upgrade_quality: 0.8
#     companion_alignment: 0.7
#     deprecated_handling: 0.5
#     namespace_migrations: 0.7
#     breaking_changes_strategy: 0.7
#     monorepo_handling: 0.8
#     user_questions_quality: 0.4
#     efficiency: 0.5
